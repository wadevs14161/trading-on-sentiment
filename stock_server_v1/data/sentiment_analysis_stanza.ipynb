{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94b7db17-4865-4520-993d-d9135279d0cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Import Data\n",
    "* For reddit_wsb.csv, no need to stem the text (The results are pretty similar, 29-Apr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c6e3bfa-9bee-4af2-a6fa-db5c1af1fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87f05137-4eaa-4651-8e78-91fb6ff34a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reddit_sentiment_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "968f1a1d-1d55-4ea6-98cb-3f7716fcf466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81835, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1fa7b13-c4d2-4ede-b4a7-fb29a4e9a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spaces in stock string\n",
    "df['stock'] = df['stock'].str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a2f7129-a64b-434f-9b03-ae67038efa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stock ticker longer than 5 (mis-extracted by Gemini)\n",
    "# It would also removed row with nan\n",
    "df = df[df['stock'].str.len() <= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa4e5701-6387-4bdf-9927-35758cc42b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"$\" in stock ticker\n",
    "df['stock'] = df['stock'].str.replace(r'\\$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c8ab341-6b5a-4010-b708-b6ee721f9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('reddit_sentiment_data_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c5f20b-3fcf-4bbc-9549-81dd51859198",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data preprocessing\n",
    "* Remove rows when title and body are null\n",
    "* Change format for timstamp to only date (%Y-%m-%d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3989ffb-1c55-4b94-92be-f9c20702c8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55546, 8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "515ed918-d66a-4de4-995b-0db9492c3a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['title'].notnull() | df['body'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37ec6b34-c3f9-483f-acca-f947cf705e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55546, 8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f730e667-e944-4f5a-b218-d6837020ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfdc130-1da7-4b08-97c4-4b8959b08e37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Ticker extraction\n",
    "* Extract stocker ticker from title and body and add a new column to store mentioned ticker\n",
    "    * Extracts potential tickers (all caps, 1-5 letters)\n",
    "    * Extracts potential tickers (Some people typed ${ticker} in title or body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2be9216a-bbe8-4e2d-af7e-db70e4c855ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55546, 8)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "729b6ff4-8b2e-4257-aa02-f81aa8534193",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = list()\n",
    "ticker_list_file = 'ticker_list.txt'\n",
    "with open(ticker_list_file, 'r') as file:\n",
    "    for line in file:\n",
    "        ticker = line.strip()\n",
    "        if ticker:\n",
    "            ticker_list.append(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21bce0ad-edb4-48af-b7ef-e2613b970932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n"
     ]
    }
   ],
   "source": [
    "print(\"YES\") if \"GOOGL\" in ticker_list else print(\"NO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "13bd73b6-5443-46b3-81be-6adc2869c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_potential_tickers(text) -> list:\n",
    "    import re\n",
    "    \"\"\"\n",
    "    Extracts potential stock tickers (all caps, 2-5 letters) from a text,\n",
    "    skipping titles that are entirely in uppercase.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Regex to find sequences of 1 to 5 uppercase letters\n",
    "        tickers = re.findall(r'\\b[A-Z]{2,5}\\b', text)\n",
    "        return tickers\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ba7f32b2-751d-4801-9835-0476077e412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tickers_in_row(row) -> list:\n",
    "    \"\"\"\n",
    "    Helper function to process each row of the DataFrame.\n",
    "    \"\"\"\n",
    "    title = row['title'] if 'title' in row.index and pd.notnull(row['title']) else ''\n",
    "    body = row['body'] if 'body' in row.index and pd.notnull(row['body']) else ''\n",
    "\n",
    "    potential_tickers_title_list = extract_potential_tickers(title)\n",
    "    potential_tickers_body_list = extract_potential_tickers(body)\n",
    "\n",
    "    # Combine and filter for real tickers in one step\n",
    "    real_tickers_in_row = [\n",
    "        ticker for ticker in (potential_tickers_title_list + potential_tickers_body_list) if ticker in ticker_list\n",
    "    ]\n",
    "\n",
    "    # Convert to set for uniqueness, then back to list\n",
    "    return list(set(real_tickers_in_row))\n",
    "real_tickers_per_row = list(zip(df.index, df.apply(find_tickers_in_row, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fa82ad3b-0cd2-4d91-a02c-9158d60ec005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>body</th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thanks for the gains Vlad!</td>\n",
       "      <td>233</td>\n",
       "      <td>41</td>\n",
       "      <td>This will go even higher with SP inclusion.</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YOLO nvda calls</td>\n",
       "      <td>148</td>\n",
       "      <td>155</td>\n",
       "      <td>Could be the best decision of my life or the w...</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What a week üòÆ‚Äçüí®</td>\n",
       "      <td>115</td>\n",
       "      <td>21</td>\n",
       "      <td>3rd straight week of trading GLD\\n+24,846 The ...</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Started scalping once I hit 25k. No looking ba...</td>\n",
       "      <td>425</td>\n",
       "      <td>251</td>\n",
       "      <td>Hyper scalping SPX, 100+ trades in 2 days</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>momey</td>\n",
       "      <td>549</td>\n",
       "      <td>76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>Unh puts are they going to print ?</td>\n",
       "      <td>153</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-14</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>ETORO IPO 175k YOLO - Fish Chart Spotted</td>\n",
       "      <td>141</td>\n",
       "      <td>23</td>\n",
       "      <td>Today, I present to you: E-Toro (Electronic Bl...</td>\n",
       "      <td>2025-05-14</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>Foot Locker Surges 65% After-Hours on Reported...</td>\n",
       "      <td>329</td>\n",
       "      <td>107</td>\n",
       "      <td>* No paywall: [https://finance.yahoo.com/news/...</td>\n",
       "      <td>2025-05-14</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>Part 2: 11 days doubled portfolio (basis $1,00...</td>\n",
       "      <td>9</td>\n",
       "      <td>33</td>\n",
       "      <td>Original post linked below, wanted to update y...</td>\n",
       "      <td>2025-05-14</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>Fuck it, we ball‚Ä¶ spy dec 19 calls</td>\n",
       "      <td>20</td>\n",
       "      <td>56</td>\n",
       "      <td>I started with options a month back and been o...</td>\n",
       "      <td>2025-05-14</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>780 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  score  comms_num  \\\n",
       "0                           Thanks for the gains Vlad!    233         41   \n",
       "1                                      YOLO nvda calls    148        155   \n",
       "2                                      What a week üòÆ‚Äçüí®    115         21   \n",
       "3    Started scalping once I hit 25k. No looking ba...    425        251   \n",
       "4                                                momey    549         76   \n",
       "..                                                 ...    ...        ...   \n",
       "536                 Unh puts are they going to print ?    153         87   \n",
       "537           ETORO IPO 175k YOLO - Fish Chart Spotted    141         23   \n",
       "538  Foot Locker Surges 65% After-Hours on Reported...    329        107   \n",
       "539  Part 2: 11 days doubled portfolio (basis $1,00...      9         33   \n",
       "540                 Fuck it, we ball‚Ä¶ spy dec 19 calls     20         56   \n",
       "\n",
       "                                                  body        date stock  \n",
       "0          This will go even higher with SP inclusion.  2025-05-31  None  \n",
       "1    Could be the best decision of my life or the w...  2025-05-31  None  \n",
       "2    3rd straight week of trading GLD\\n+24,846 The ...  2025-05-31  None  \n",
       "3            Hyper scalping SPX, 100+ trades in 2 days  2025-05-31  None  \n",
       "4                                                  NaN  2025-05-31  None  \n",
       "..                                                 ...         ...   ...  \n",
       "536                                                NaN  2025-05-14  None  \n",
       "537  Today, I present to you: E-Toro (Electronic Bl...  2025-05-14     E  \n",
       "538  * No paywall: [https://finance.yahoo.com/news/...  2025-05-14  None  \n",
       "539  Original post linked below, wanted to update y...  2025-05-14  None  \n",
       "540  I started with options a month back and been o...  2025-05-14  None  \n",
       "\n",
       "[780 rows x 6 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new DataFrame to store the results with duplicated rows\n",
    "new_rows = list()\n",
    "for index, real_tickers in real_tickers_per_row:\n",
    "    original_row = df.iloc[index]\n",
    "    if real_tickers:\n",
    "        for ticker in real_tickers:\n",
    "            new_row = original_row.copy()\n",
    "            new_row['stock'] = ticker\n",
    "            new_rows.append(new_row)\n",
    "    else:\n",
    "        new_row = original_row.copy()\n",
    "        new_row['stock'] = None  # Or some other indicator for no real ticker\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "df_tickers_found = pd.DataFrame(new_rows)\n",
    "df_tickers_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "98f5d73c-1a0d-407c-b6df-72608104c43a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>body</th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNH YOLO &amp; Thesis</td>\n",
       "      <td>192</td>\n",
       "      <td>107</td>\n",
       "      <td>I'm betting over half my port on UNH. All shar...</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>UNH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNH YOLO &amp; Thesis</td>\n",
       "      <td>192</td>\n",
       "      <td>107</td>\n",
       "      <td>I'm betting over half my port on UNH. All shar...</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150k gain on RKLB</td>\n",
       "      <td>164</td>\n",
       "      <td>30</td>\n",
       "      <td>Finally decided to sell my RKLB position</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>RKLB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YTD check-in</td>\n",
       "      <td>1213</td>\n",
       "      <td>209</td>\n",
       "      <td>They were mainly puts and some calls for SPY a...</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>INTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unite for UNH</td>\n",
       "      <td>98</td>\n",
       "      <td>122</td>\n",
       "      <td>Gather all my fellow UNH believers we have Hem...</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>UNH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>welfare check: still retarded. doubled down on...</td>\n",
       "      <td>124</td>\n",
       "      <td>157</td>\n",
       "      <td>‚Äúif you can‚Äôt handle a 50% drop, you shouldn‚Äôt...</td>\n",
       "      <td>2025-05-14</td>\n",
       "      <td>UNH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>welfare check: still retarded. doubled down on...</td>\n",
       "      <td>124</td>\n",
       "      <td>157</td>\n",
       "      <td>‚Äúif you can‚Äôt handle a 50% drop, you shouldn‚Äôt...</td>\n",
       "      <td>2025-05-14</td>\n",
       "      <td>HTZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>UnitedHealth Group Is Under Criminal Investiga...</td>\n",
       "      <td>2555</td>\n",
       "      <td>184</td>\n",
       "      <td>$UNH down 7% after market. Nothing cheers more...</td>\n",
       "      <td>2025-05-14</td>\n",
       "      <td>UNH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>UnitedHealth Group Is Under Criminal Investiga...</td>\n",
       "      <td>2555</td>\n",
       "      <td>184</td>\n",
       "      <td>$UNH down 7% after market. Nothing cheers more...</td>\n",
       "      <td>2025-05-14</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>ETORO IPO 175k YOLO - Fish Chart Spotted</td>\n",
       "      <td>141</td>\n",
       "      <td>23</td>\n",
       "      <td>Today, I present to you: E-Toro (Electronic Bl...</td>\n",
       "      <td>2025-05-14</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>505 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  score  comms_num  \\\n",
       "0                                    UNH YOLO & Thesis    192        107   \n",
       "1                                    UNH YOLO & Thesis    192        107   \n",
       "2                                    150k gain on RKLB    164         30   \n",
       "3                                         YTD check-in   1213        209   \n",
       "4                                        Unite for UNH     98        122   \n",
       "..                                                 ...    ...        ...   \n",
       "500  welfare check: still retarded. doubled down on...    124        157   \n",
       "501  welfare check: still retarded. doubled down on...    124        157   \n",
       "502  UnitedHealth Group Is Under Criminal Investiga...   2555        184   \n",
       "503  UnitedHealth Group Is Under Criminal Investiga...   2555        184   \n",
       "504           ETORO IPO 175k YOLO - Fish Chart Spotted    141         23   \n",
       "\n",
       "                                                  body        date stock  \n",
       "0    I'm betting over half my port on UNH. All shar...  2025-05-31   UNH  \n",
       "1    I'm betting over half my port on UNH. All shar...  2025-05-31     A  \n",
       "2             Finally decided to sell my RKLB position  2025-05-31  RKLB  \n",
       "3    They were mainly puts and some calls for SPY a...  2025-05-31  INTC  \n",
       "4    Gather all my fellow UNH believers we have Hem...  2025-05-31   UNH  \n",
       "..                                                 ...         ...   ...  \n",
       "500  ‚Äúif you can‚Äôt handle a 50% drop, you shouldn‚Äôt...  2025-05-14   UNH  \n",
       "501  ‚Äúif you can‚Äôt handle a 50% drop, you shouldn‚Äôt...  2025-05-14   HTZ  \n",
       "502  $UNH down 7% after market. Nothing cheers more...  2025-05-14   UNH  \n",
       "503  $UNH down 7% after market. Nothing cheers more...  2025-05-14     L  \n",
       "504  Today, I present to you: E-Toro (Electronic Bl...  2025-05-14     E  \n",
       "\n",
       "[505 rows x 6 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure ticker_list is a set for efficient lookups\n",
    "ticker_set = set(ticker_list)\n",
    "\n",
    "# Get the subset of df_tickers_found where 'stock' is null\n",
    "df_null_stock = df_tickers_found[df_tickers_found['stock'].isnull()]\n",
    "\n",
    "all_new_rows_from_dollar_sign = []\n",
    "\n",
    "# Define a function to process text for $tickers\n",
    "def extract_and_validate_dollar_tickers(text_content) -> list:\n",
    "    if not isinstance(text_content, str) or '$' not in text_content:\n",
    "        return [] # No tickers found or not a string\n",
    "\n",
    "    potential_tickers = re.findall(r'\\$(\\w+)', text_content)\n",
    "    real_tickers = set()\n",
    "    for potential_ticker in potential_tickers:\n",
    "        # Check if the potential ticker is not entirely digits and is in our valid ticker_set\n",
    "        if not any(char.isdigit() for char in potential_ticker) and potential_ticker in ticker_set:\n",
    "            real_tickers.add(potential_ticker)\n",
    "    return list(real_tickers)\n",
    "\n",
    "# Iterate once over the relevant rows from the original df's index\n",
    "# It's better to get original rows from `df` itself based on the index from `df_null_stock`\n",
    "for original_df_index, row_data in df_null_stock.iterrows():\n",
    "    title_content = row_data['title']\n",
    "    body_content = row_data['body']\n",
    "\n",
    "    # Extract tickers from title\n",
    "    real_tickers_from_title = extract_and_validate_dollar_tickers(title_content)\n",
    "    # Extract tickers from body\n",
    "    real_tickers_from_body = extract_and_validate_dollar_tickers(body_content)\n",
    "\n",
    "    # Combine unique tickers from both\n",
    "    combined_real_tickers = list(set(real_tickers_from_title + real_tickers_from_body))\n",
    "\n",
    "    if combined_real_tickers:\n",
    "        # Get the original row data from the main `df` using its index\n",
    "        # This ensures we get the original columns, not just those in df_tickers_found\n",
    "        original_source_row = df.loc[original_df_index].to_dict()\n",
    "\n",
    "        for ticker in combined_real_tickers:\n",
    "            new_row_dict = original_source_row.copy()\n",
    "            new_row_dict['stock'] = ticker\n",
    "            all_new_rows_from_dollar_sign.append(new_row_dict)\n",
    "\n",
    "# Create the DataFrame once at the end\n",
    "df_found_from_dollar_sign = pd.DataFrame(all_new_rows_from_dollar_sign)\n",
    "\n",
    "# Ensure 'stock' column is at the front if the DataFrame is not empty\n",
    "if not df_found_from_dollar_sign.empty:\n",
    "    cols = ['stock'] + [col for col in df_found_from_dollar_sign.columns if col != 'stock']\n",
    "    df_found_from_dollar_sign = df_found_from_dollar_sign[cols]\n",
    "\n",
    "df_with_tickers = pd.concat([df_tickers_found[df_tickers_found['stock'].notnull()], df_found_from_dollar_sign]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c3ce7-eca5-4a58-8dbc-7a0d92fe973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with ticker is single character\n",
    "df_with_tickers = df_with_tickers[df_with_tickers['stock'].str.len() > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81f491-09c2-4b3d-a39d-21f4d6849b9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Natural Language Processing\n",
    "#### Use Stanza library\n",
    "* Remove stop words\n",
    "* Stemming words\n",
    "* Get sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a0d00611-3bd5-4572-ba4d-282d8cf7e171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenshinluo/Documents/pyvenv/venv_stock/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import stanza\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "142fd832-46b8-4b00-a9c7-b4b00bb1aa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-06-15 12:07:01 INFO: Downloaded file to /Users/wenshinluo/stanza_resources/resources.json\n",
      "2025-06-15 12:07:01 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-06-15 12:07:02 INFO: File exists: /Users/wenshinluo/stanza_resources/en/default.zip\n",
      "2025-06-15 12:07:06 INFO: Finished downloading models and saved to /Users/wenshinluo/stanza_resources\n",
      "2025-06-15 12:07:06 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "\n",
      "2025-06-15 12:07:06 INFO: Downloaded file to /Users/wenshinluo/stanza_resources/resources.json\n",
      "2025-06-15 12:07:06 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-06-15 12:07:06 INFO: Loading these models for language: en (English):\n",
      "==============================\n",
      "| Processor | Package        |\n",
      "------------------------------\n",
      "| tokenize  | combined       |\n",
      "| mwt       | combined       |\n",
      "| sentiment | sstplus_charlm |\n",
      "==============================\n",
      "\n",
      "2025-06-15 12:07:06 INFO: Using device: cpu\n",
      "2025-06-15 12:07:06 INFO: Loading: tokenize\n",
      "2025-06-15 12:07:08 INFO: Loading: mwt\n",
      "2025-06-15 12:07:08 INFO: Loading: sentiment\n",
      "2025-06-15 12:07:10 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Stanza\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', processors='tokenize, sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4a1cb76d-81ad-4701-917a-983569556dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmer\n",
    "port_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9f390635-3a4e-4c65-b447-35b5e45983aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment score function\n",
    "def sentiment_score(text):\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "        if doc.sentences:\n",
    "            return doc.sentences[0].sentiment\n",
    "        else:\n",
    "            return None  # Handle empty strings or cases with no sentences\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: '{text}' - {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "453f1bea-fbfc-466b-8fe8-cabe0de250ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 19s, sys: 2.04 s, total: 1min 21s\n",
      "Wall time: 43.9 s\n"
     ]
    }
   ],
   "source": [
    "%time df_with_tickers['title_sentiment'] = df_with_tickers['title'].apply(sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a53696e6-752b-41b3-8d61-02758c92ebac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>body</th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "      <th>title_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNH YOLO &amp; Thesis</td>\n",
       "      <td>192</td>\n",
       "      <td>107</td>\n",
       "      <td>I'm betting over half my port on UNH. All shar...</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>UNH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNH YOLO &amp; Thesis</td>\n",
       "      <td>192</td>\n",
       "      <td>107</td>\n",
       "      <td>I'm betting over half my port on UNH. All shar...</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150k gain on RKLB</td>\n",
       "      <td>164</td>\n",
       "      <td>30</td>\n",
       "      <td>Finally decided to sell my RKLB position</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>RKLB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YTD check-in</td>\n",
       "      <td>1213</td>\n",
       "      <td>209</td>\n",
       "      <td>They were mainly puts and some calls for SPY a...</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>INTC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unite for UNH</td>\n",
       "      <td>98</td>\n",
       "      <td>122</td>\n",
       "      <td>Gather all my fellow UNH believers we have Hem...</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>UNH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>welfare check: still retarded. doubled down on...</td>\n",
       "      <td>124</td>\n",
       "      <td>157</td>\n",
       "      <td>‚Äúif you can‚Äôt handle a 50% drop, you shouldn‚Äôt...</td>\n",
       "      <td>2025-05-14</td>\n",
       "      <td>UNH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>welfare check: still retarded. doubled down on...</td>\n",
       "      <td>124</td>\n",
       "      <td>157</td>\n",
       "      <td>‚Äúif you can‚Äôt handle a 50% drop, you shouldn‚Äôt...</td>\n",
       "      <td>2025-05-14</td>\n",
       "      <td>HTZ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>UnitedHealth Group Is Under Criminal Investiga...</td>\n",
       "      <td>2555</td>\n",
       "      <td>184</td>\n",
       "      <td>$UNH down 7% after market. Nothing cheers more...</td>\n",
       "      <td>2025-05-14</td>\n",
       "      <td>UNH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>UnitedHealth Group Is Under Criminal Investiga...</td>\n",
       "      <td>2555</td>\n",
       "      <td>184</td>\n",
       "      <td>$UNH down 7% after market. Nothing cheers more...</td>\n",
       "      <td>2025-05-14</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>ETORO IPO 175k YOLO - Fish Chart Spotted</td>\n",
       "      <td>141</td>\n",
       "      <td>23</td>\n",
       "      <td>Today, I present to you: E-Toro (Electronic Bl...</td>\n",
       "      <td>2025-05-14</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>505 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  score  comms_num  \\\n",
       "0                                    UNH YOLO & Thesis    192        107   \n",
       "1                                    UNH YOLO & Thesis    192        107   \n",
       "2                                    150k gain on RKLB    164         30   \n",
       "3                                         YTD check-in   1213        209   \n",
       "4                                        Unite for UNH     98        122   \n",
       "..                                                 ...    ...        ...   \n",
       "500  welfare check: still retarded. doubled down on...    124        157   \n",
       "501  welfare check: still retarded. doubled down on...    124        157   \n",
       "502  UnitedHealth Group Is Under Criminal Investiga...   2555        184   \n",
       "503  UnitedHealth Group Is Under Criminal Investiga...   2555        184   \n",
       "504           ETORO IPO 175k YOLO - Fish Chart Spotted    141         23   \n",
       "\n",
       "                                                  body        date stock  \\\n",
       "0    I'm betting over half my port on UNH. All shar...  2025-05-31   UNH   \n",
       "1    I'm betting over half my port on UNH. All shar...  2025-05-31     A   \n",
       "2             Finally decided to sell my RKLB position  2025-05-31  RKLB   \n",
       "3    They were mainly puts and some calls for SPY a...  2025-05-31  INTC   \n",
       "4    Gather all my fellow UNH believers we have Hem...  2025-05-31   UNH   \n",
       "..                                                 ...         ...   ...   \n",
       "500  ‚Äúif you can‚Äôt handle a 50% drop, you shouldn‚Äôt...  2025-05-14   UNH   \n",
       "501  ‚Äúif you can‚Äôt handle a 50% drop, you shouldn‚Äôt...  2025-05-14   HTZ   \n",
       "502  $UNH down 7% after market. Nothing cheers more...  2025-05-14   UNH   \n",
       "503  $UNH down 7% after market. Nothing cheers more...  2025-05-14     L   \n",
       "504  Today, I present to you: E-Toro (Electronic Bl...  2025-05-14     E   \n",
       "\n",
       "     title_sentiment  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  1  \n",
       "3                  1  \n",
       "4                  1  \n",
       "..               ...  \n",
       "500                0  \n",
       "501                0  \n",
       "502                1  \n",
       "503                1  \n",
       "504                1  \n",
       "\n",
       "[505 rows x 7 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d598b8db-6ebc-4fd7-9452-1454ca9c352a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "Error processing text: 'nan' - input should be either str, list or Document\n",
      "CPU times: user 25min 16s, sys: 1min 29s, total: 26min 46s\n",
      "Wall time: 14min 57s\n"
     ]
    }
   ],
   "source": [
    "%time df_with_tickers['body_sentiment'] = df_with_tickers['body'].apply(sentiment_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ed019-8dff-483e-9954-1cdec3fcf0ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Post Data processing\n",
    "* Adjust mistaken sentiment score for null post body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e07a4b42-f1c2-4b64-9c25-57f4f92618c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readjust unknown assignment of sentiment score to Null body\n",
    "df_with_tickers.loc[df_with_tickers['body'].isnull(), 'body_sentiment'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d054a4b1-5d38-4ec5-933f-c110c5687e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_tickers.to_csv('reddit_sentiment_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
