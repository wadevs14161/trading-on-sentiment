{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7f7be03-1d61-4a55-92c1-06b9e7be269f",
   "metadata": {
    "id": "a7f7be03-1d61-4a55-92c1-06b9e7be269f"
   },
   "source": [
    "### Load raw data from Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ce11743-60ba-4575-9540-25e34b411be6",
   "metadata": {
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1751266803550,
     "user": {
      "displayName": "Wenshin Luo",
      "userId": "03239099618070958584"
     },
     "user_tz": -60
    },
    "id": "7ce11743-60ba-4575-9540-25e34b411be6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "waFZxEgM0nCT",
   "metadata": {
    "executionInfo": {
     "elapsed": 1011,
     "status": "ok",
     "timestamp": 1751266804564,
     "user": {
      "displayName": "Wenshin Luo",
      "userId": "03239099618070958584"
     },
     "user_tz": -60
    },
    "id": "waFZxEgM0nCT"
   },
   "outputs": [],
   "source": [
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d2170c0-b5e1-4ae7-9e24-ab135078acaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1554,
     "status": "ok",
     "timestamp": 1751266806120,
     "user": {
      "displayName": "Wenshin Luo",
      "userId": "03239099618070958584"
     },
     "user_tz": -60
    },
    "id": "7d2170c0-b5e1-4ae7-9e24-ab135078acaf",
    "outputId": "e8158dee-52bb-47e2-ab4c-4375f0c70570"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92072"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('reddit_sentiment_data_unstemmed.csv')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72117b1b-6f0a-47d5-8548-9bd3d21ccafe",
   "metadata": {
    "id": "72117b1b-6f0a-47d5-8548-9bd3d21ccafe"
   },
   "source": [
    "### Send title and body to Gemini API\n",
    "* Extract stock mentioned in title or body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef8e22d1-05cc-46de-babb-6f46822054c1",
   "metadata": {
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1751266806331,
     "user": {
      "displayName": "Wenshin Luo",
      "userId": "03239099618070958584"
     },
     "user_tz": -60
    },
    "id": "ef8e22d1-05cc-46de-babb-6f46822054c1"
   },
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=\"\")\n",
    "\n",
    "def find_ticker(text) -> list:\n",
    "    if isinstance(text, str):\n",
    "        tickers = []\n",
    "        prompt=f'''\n",
    "            From given text \"{text}\", try to find US stock ticker\n",
    "            Instruction:\n",
    "            - If found multiple tickers, use comma to separate, do not include \"\\n\", example: TSLA, AAPL\n",
    "            - return in string format\n",
    "            - if not found, return just \"nan\"\n",
    "            '''\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash-lite\",\n",
    "            contents=prompt\n",
    "        )\n",
    "        # print(text)\n",
    "        # print(response.text)\n",
    "        res = response.text\n",
    "        ts = res.split(',')\n",
    "        for t in ts:\n",
    "            tickers.append(t)\n",
    "        return tickers\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bbffe8f-e605-4c21-b51a-e59a281ebec9",
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1751266806347,
     "user": {
      "displayName": "Wenshin Luo",
      "userId": "03239099618070958584"
     },
     "user_tz": -60
    },
    "id": "0bbffe8f-e605-4c21-b51a-e59a281ebec9",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def find_tickers_in_row(row) -> list:\n",
    "    \"\"\"\n",
    "    Helper function to process each row of the DataFrame.\n",
    "    \"\"\"\n",
    "    title = row['title'] if 'title' in row.index and pd.notnull(row['title']) else ''\n",
    "    body = row['body'] if 'body' in row.index and pd.notnull(row['body']) else ''\n",
    "\n",
    "    potential_tickers_title_list = find_ticker(title)\n",
    "    time.sleep(1)\n",
    "    potential_tickers_body_list = find_ticker(body)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Combine and filter for real tickers in one step\n",
    "    real_tickers_in_row = potential_tickers_title_list + potential_tickers_body_list\n",
    "\n",
    "    # Convert to set for uniqueness, then back to list\n",
    "    return list(set(real_tickers_in_row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e9193b5-54b9-4f02-8fd8-e8c493d2881a",
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 22205369,
     "status": "ok",
     "timestamp": 1751289011720,
     "user": {
      "displayName": "Wenshin Luo",
      "userId": "03239099618070958584"
     },
     "user_tz": -60
    },
    "id": "0e9193b5-54b9-4f02-8fd8-e8c493d2881a",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Done! (90 min one 2000 rows), 92072 rows total\n",
    "for i in range(84000, 92072, 2000):\n",
    "    df_part = df.iloc[i:i+2000]\n",
    "    real_tickers_per_row = list(zip(df_part.index, df_part.apply(find_tickers_in_row, axis=1)))\n",
    "\n",
    "    # Create a new DataFrame to store the results with duplicated rows\n",
    "    new_rows = list()\n",
    "    for index, real_tickers in real_tickers_per_row:\n",
    "        original_row = df.iloc[index]\n",
    "        if real_tickers:\n",
    "            for ticker in real_tickers:\n",
    "                new_row = original_row.copy()\n",
    "                new_row['stock_gemini'] = ticker\n",
    "                new_rows.append(new_row)\n",
    "        else:\n",
    "            new_row = original_row.copy()\n",
    "            new_row['stock_gemini'] = None  # Or some other indicator for no real ticker\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "    df_tickers_found = pd.DataFrame(new_rows)\n",
    "    df_tickers_found['stock_gemini'] = df_tickers_found['stock_gemini'].astype(str)\n",
    "    rows_to_remove_mask = df_tickers_found['stock_gemini'].str.contains('nan', case=False, na=False)\n",
    "    df_processed = df_tickers_found[~rows_to_remove_mask].copy()\n",
    "    df_processed['stock_gemini'] = df_processed['stock_gemini'].str.replace('\\n', '', regex=False)\n",
    "    df_processed.to_csv(f'df_found_ticker_{i}.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
